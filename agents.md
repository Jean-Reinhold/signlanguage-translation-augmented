# agents.md â€” Sign Language Translation with Data Augmentation

> **Project**: Sign Language Translation Pipeline with LLM-based Data Augmentation  
> **Author**: Pedro Dal Bianco  
> **Status**: Active Development (Master's Thesis Research)

---

## ðŸŽ¯ Project Goal

This repository implements an **augmented training pipeline for Sign Language Translation (SLT)** based on the **Signformer** architecture. The core hypothesis being tested is:

> *It is possible to improve **generalization** in SLT by training a single keypoint-based model across multiple languages and corpora, supported by carefully controlled **textual and kinematic data augmentation**.*

The project advances three integrated fronts:

1. **Multi-corpora Integration** â€” Standardizing heterogeneous SLT datasets (PHOENIX-2014T, LSA-T, How2Sign, LSFB-CONT, ISLTranslate, GSL) into a unified manifest format
2. **Multilingual Gloss-free Training** â€” Training a single Signformer model on multiple sign languages using pose keypoints (no intermediate gloss annotations)
3. **Data Augmentation for Generalization** â€” Evaluating the impact of LLM-based text augmentation (paraphrases, back-translation) and kinematic pose perturbations on cross-domain/cross-signer performance

---

## ðŸ—ï¸ Architecture Overview

### Why Signformer?

The **Signformer** model operates on **pose keypoints** (extracted via MediaPipe Holistic) rather than raw video frames. This design choice provides:

- **Dimensionality Reduction**: Keypoints compress visual input to essential geometry/motion
- **Invariance**: Robust to lighting, background, camera angle, and appearance variations
- **Portability**: Enables training across heterogeneous datasets with different capture conditions
- **Interpretability**: Attention maps over keypoints reveal which body regions contribute to predictions
- **Efficiency**: Lower compute requirements than video-based models (larger batch sizes, faster convergence)

### Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DATA FLOW DIAGRAM                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Raw Video/Poses    â”‚
              â”‚  (Multiple Datasets) â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Format to SLTDataset â”‚  â† lib/slt_datasets/
              â”‚  (Unified Manifest)   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
        â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text Augment  â”‚ â”‚  Pose Aug   â”‚ â”‚ Export to     â”‚
â”‚ (LLM-based)   â”‚ â”‚ (Kinematic) â”‚ â”‚ Signformer    â”‚
â”‚ GPT-4o-mini   â”‚ â”‚             â”‚ â”‚ .pami0 format â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                â”‚                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Signformer Train   â”‚
              â”‚  (Transformer Enc/Dec)â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Evaluation         â”‚
              â”‚ (BLEU, chrF, ROUGE)  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“ Repository Structure

```
signlanguage-translation-augmented/
â”œâ”€â”€ agents.md                          # This file
â”œâ”€â”€ README.md                          # Quick start guide
â”œâ”€â”€ pyproject.toml                     # Dependencies & package config
â”œâ”€â”€ requirements.txt                   # Pip dependencies
â”œâ”€â”€ .env.example                       # Environment variables template
â”œâ”€â”€ .env                               # Local environment (gitignored)
â”‚
â”œâ”€â”€ lib/                               # LOCAL LIBRARY PACKAGES
â”‚   â”œâ”€â”€ posecraft/                     # Pose keypoint manipulation
â”‚   â”‚   â”œâ”€â”€ Pose.py                    # Pose class & operations
â”‚   â”‚   â”œâ”€â”€ transforms.py              # Data transforms (filter, pad, flatten)
â”‚   â”‚   â””â”€â”€ interpolate.py             # Frame interpolation
â”‚   â””â”€â”€ slt_datasets/                  # SLT dataset utilities
â”‚       â”œâ”€â”€ SLTDataset.py              # Unified dataset class
â”‚       â””â”€â”€ WordLevelTokenizer.py      # Text tokenization
â”‚
â”œâ”€â”€ src/                               # Main source code
â”‚   â”œâ”€â”€ augmentation/                  # TEXT AUGMENTATION
â”‚   â”‚   â”œâ”€â”€ expand_db.ipynb            # GPT-4o-mini paraphrasing
â”‚   â”‚   â””â”€â”€ train_aug.tsv              # Sample augmented data
â”‚   â”œâ”€â”€ export/                        # Dataset export utilities
â”‚   â”‚   â”œâ”€â”€ export_db.ipynb            # Export to Signformer format
â”‚   â”‚   â””â”€â”€ export_db.py               # Export script
â”‚   â”œâ”€â”€ training/                      # Custom transformer training
â”‚   â”‚   â”œâ”€â”€ train.ipynb                # PyTorch Lightning training
â”‚   â”‚   â”œâ”€â”€ KeypointsTransformer.py    # Transformer model
â”‚   â”‚   â”œâ”€â”€ LightningKeypointsTransformer.py  # Lightning wrapper
â”‚   â”‚   â”œâ”€â”€ Translator.py              # Translation utilities
â”‚   â”‚   â””â”€â”€ results_analysis.ipynb     # Analyze results
â”‚   â”œâ”€â”€ config/                        # Hyperparameter configs
â”‚   â”‚   â”œâ”€â”€ GSL.json
â”‚   â”‚   â”œâ”€â”€ LSAT.json
â”‚   â”‚   â””â”€â”€ RWTH_PHOENIX_2014T.json
â”‚   â””â”€â”€ interp/                        # Interpolation experiments
â”‚
â”œâ”€â”€ signformer/                        # SIGNFORMER MODEL (Production)
â”‚   â”œâ”€â”€ main/                          # Core modules
â”‚   â”‚   â”œâ”€â”€ training.py                # Training loop
â”‚   â”‚   â”œâ”€â”€ data.py                    # Data loading
â”‚   â”‚   â”œâ”€â”€ dataset.py                 # SignTranslationDataset
â”‚   â”‚   â”œâ”€â”€ model.py                   # Model architecture
â”‚   â”‚   â”œâ”€â”€ encoders.py                # Transformer encoder
â”‚   â”‚   â”œâ”€â”€ decoders.py                # Transformer decoder
â”‚   â”‚   â””â”€â”€ prediction.py              # Inference
â”‚   â”œâ”€â”€ configs/                       # YAML configurations
â”‚   â”‚   â”œâ”€â”€ sign.yaml                  # Main config
â”‚   â”‚   â””â”€â”€ sign_finetune.yaml         # Finetuning config
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ build_phoenix14t_ext.py    # Build augmented dataset
â”‚   â””â”€â”€ requirements.txt               # Signformer-specific deps
â”‚
â”œâ”€â”€ dataset_formatting/                # Dataset-specific formatting
â”‚   â”œâ”€â”€ rwth/                          # RWTH-Phoenix-2014T (DGSâ†’DE)
â”‚   â”œâ”€â”€ gsl/                           # Greek Sign Language (GSLâ†’EL)
â”‚   â”œâ”€â”€ lsat/                          # LSA-T Argentinian (LSAâ†’ES)
â”‚   â”œâ”€â”€ isl/                           # Indian Sign Language (ISLâ†’EN)
â”‚   â”œâ”€â”€ howtosign/                     # How2Sign (ASLâ†’EN)
â”‚   â””â”€â”€ lsfb-cont/                     # LSFB Continuous (LSFBâ†’FR)
â”‚
â”œâ”€â”€ explanations/                      # Documentation
â”‚   â””â”€â”€ SLT_PIPELINE_DOCUMENTATION.md  # Complete pipeline docs
â”‚
â””â”€â”€ docs/                              # Thesis and papers
    â””â”€â”€ thesis/
        â””â”€â”€ exemplo-tcc.tex            # Master's thesis (Portuguese)
```

---

## ðŸ”§ Dependencies & Setup

### Core Dependencies

| Category | Packages |
|----------|----------|
| **ML Frameworks** | `torch>=2.0`, `lightning>=2.0`, `torchmetrics` |
| **Data Processing** | `numpy<2.0`, `pandas`, `scikit-learn`, `h5py` |
| **NLP & Metrics** | `sacrebleu`, `nltk`, `rouge-score` |
| **Visualization** | `matplotlib`, `seaborn`, `tqdm` |
| **Experiment Tracking** | `wandb` |

### Optional Dependencies

```bash
# LLM augmentation (Azure OpenAI API)
pip install openai nest-asyncio python-dotenv

# Signformer training (SophiaG optimizer)
pip install sophia-optimizer

# Dataset formatting (TensorFlow Datasets)
pip install tensorflow tensorflow-datasets sign-language-datasets
```

### Environment Variables

Copy `.env.example` to `.env` and configure:

```bash
cp .env.example .env
# Edit .env with your Azure OpenAI credentials
```

| Variable | Description |
|----------|-------------|
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI resource endpoint |
| `AZURE_OPENAI_API_KEY` | API key for authentication |
| `AZURE_OPENAI_API_VERSION` | API version (e.g., `2025-04-01-preview`) |
| `AZURE_OPENAI_DEPLOYMENT` | Model deployment name (e.g., `gpt-5-mini`) |

### Quick Setup

```bash
# Using uv (recommended)
./scripts/setup.sh --all

# Manual setup
uv venv --python 3.11
source .venv/bin/activate
uv pip install -e ".[all]"
```

---

## ðŸ“Š Available Datasets

| Dataset | Sign Language | Text Language | Samples | Domain |
|---------|---------------|---------------|---------|--------|
| **RWTH-PHOENIX-2014T** | German (DGS) | German | 8,257 | TV Weather |
| **LSA-T** | Argentine (LSA) | Spanish | 14,880 | News |
| **How2Sign** | American (ASL) | English | 35,191 | How-to tutorials |
| **LSFB-CONT** | Belgian French (LSFB) | French | 27,500 | Narratives |
| **ISLTranslate** | Indian (ISL) | English | 31,222 | Educational |
| **GSL Continuous** | Greek (GSL) | Greek | 40,826 | Daily phrases |

### Data Format

Each dataset follows the unified `SLTDataset` structure:

```
{DATASET}/
â”œâ”€â”€ metadata.json         # Dataset metadata
â”œâ”€â”€ annotations.csv       # id, text, gloss, signer, split
â””â”€â”€ poses/
    â””â”€â”€ {id}.npy          # Shape: (frames, people, keypoints, coords)
```

**Pose keypoints**: MediaPipe Holistic (543 landmarks: 468 face + 33 body + 21Ã—2 hands)

---

## ðŸš€ Data Augmentation Methodology

### 1. Text Augmentation with LLMs

**Location**: `src/augmentation/expand_db.ipynb`

The pipeline uses **Azure OpenAI GPT-5-mini** to generate semantically equivalent paraphrases:

```python
# Load from .env file
from dotenv import load_dotenv
import os

load_dotenv()

# Azure OpenAI Configuration
AZURE_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION")
DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT")  # gpt-5-mini

SYSTEM_PROMPT = """
You are a helpful assistant rewriting sentences.
For every user input, produce {n} paraphrases that:
â€¢ Preserve the exact meaning, tense and register.
â€¢ Reuse at least 70% of the original words.
â€¢ Vary mainly through word order or minor synonym substitutions.
â€¢ Keep length within Â±3 tokens of the original.
â€¢ Do NOT add or omit information.
"""

# Configuration
AUG_FACTOR = 2          # Paraphrases per original
BATCH_SIZE = 8          # Concurrent API requests
TEMPERATURE = 0.8       # Controlled diversity
```

**Result**: RWTH-Phoenix training set expanded from 7,096 â†’ 21,288 samples (3Ã—)

### 2. Back-Translation Strategy (Thesis Plan)

**Pivot Language Policy**:
- For non-EN/DE targets â†’ Two pivots: `tâ†’ENâ†’t` and `tâ†’DEâ†’t`
- For EN/DE targets â†’ Spanish pivot: `ENâ†’ESâ†’EN` or `DEâ†’ESâ†’DE`

This creates systematic cross-lingual coupling to improve generalization.

### 3. Validation & Filtering

Generated variants are validated using:

| Metric | Purpose |
|--------|---------|
| **SBERT Cosine** | Semantic similarity threshold |
| **chrF/chrF++** | Surface-level distance control |
| **BERTScore** | Semantic verification |
| **Deduplication** | n-gram/minhash filtering |

### 4. Kinematic Pose Augmentation (Planned)

Perturbations applied to keypoint sequences:

- **Geometric**: Translation, scaling, rotation (plane)
- **Temporal**: Time warping, random sampling, padding
- **Dropout**: Keypoint occlusion with interpolation
- **Constraints**: Bone-length consistency validation

```python
# Plausibility check
Ï†(K) = (1/|B|T) Î£_t Î£_(p,q)âˆˆB |â€–pÌ‚_t - qÌ‚_tâ€–â‚‚ - â„“Ì„_pq|
# Accept if Ï†(K) â‰¤ Ï„ (bone-length deviation threshold)
```

---

## ðŸ‹ï¸ Training Pipeline

### Using Signformer

```bash
cd signformer/

# 1. Build augmented dataset
python scripts/build_phoenix14t_ext.py \
    --data_dir PHOENIX2014T \
    --tsv train_aug.tsv \
    --out_prefix phoenix14t-ext.pami0

# 2. Train
python -m main train configs/sign.yaml --gpu_id 0

# 3. Evaluate
python -m main test configs/sign.yaml --ckpt path/to/best.ckpt
```

### Configuration (sign.yaml)

```yaml
data:
    data_path: ../PHOENIX2014T
    train: phoenix14t-ext.pami0.train    # Augmented
    dev: phoenix14t.pami0.dev
    test: phoenix14t.pami0.test
    feature_size: 1024                    # 512 keypoints Ã— 2 coords

training:
    optimizer: sophiag
    learning_rate: 0.0004
    batch_size: 32
    epochs: 1000
    validation_freq: 100
    recognition_loss_weight: 0.0          # Translation-only (gloss-free)
    translation_loss_weight: 1.0

model:
    encoder:
        type: transformer
        num_layers: 1
        num_heads: 8
        hidden_size: 256
    decoder:
        type: transformer
        num_layers: 1
        num_heads: 8
```

### Using Custom Transformer (Alternative)

```bash
cd src/training/
jupyter notebook train.ipynb
```

---

## ðŸ“ˆ Evaluation Metrics

| Metric | Description |
|--------|-------------|
| **BLEU-4** | Primary metric (n-gram precision with brevity penalty) |
| **chrF/chrF++** | Character-level F-score (morphologically rich languages) |
| **ROUGE** | Recall-oriented summary evaluation |
| **BERTScore** | Semantic similarity via embeddings |

### Evaluation Protocols

1. **In-domain**: Train/test on same corpus
2. **Cross-domain**: Train on corpus A, test on corpus B
3. **Signer-held-out**: Exclude specific signers from training

---

## ðŸ“š Key References

### Architecture
- **Signformer**: Yang, E. (2024). *Signformer is all you need: Towards Edge AI for Sign Language*. arXiv:2411.12901

### Datasets
- **PHOENIX-2014T**: Forster et al. (2014), Koller et al. (2015)
- **LSA-T**: Dal Bianco et al. (2022)
- **How2Sign**: Duarte et al. (CVPR 2021)
- **LSFB-CONT**: Fink et al. (2021)
- **ISLTranslate**: Joshi et al. (ACL Findings 2023)
- **GSL**: Adaloglou et al. (2020)

### Data Augmentation
- **Back-translation**: Sennrich et al. (2016), Edunov et al. (2018)
- **ParaNMT**: Wieting & Gimpel (2018)
- **LLM Data Aug**: Ding et al. (2024), ChatGPT DA (2023)
- **PoseAug**: Gong et al. (2021)

### Pose Extraction
- **MediaPipe Holistic**: Lugaresi et al. (2019)

---

## ðŸ—‚ï¸ External Dependencies

### Required Repositories

| Repository | Purpose | Location |
|------------|---------|----------|
| `posecraft` | Pose manipulation library | `lib/posecraft/` (local copy) |
| `slt_datasets` | Unified dataset interface | `lib/slt_datasets/` (local copy) |

### Data Locations (Local Machine)

| Path | Content |
|------|---------|
| `/mnt/disk3Tb/slt-datasets/` | Raw formatted datasets |
| `/mnt/disk3Tb/augmented-slt-datasets/` | Text-augmented datasets |
| `/mnt/disk3Tb/exported-slt-datasets/` | Signformer format (`.pami0.*`) |

---

## ðŸ”¬ Research Questions

This project aims to answer:

1. **Does LLM-based text augmentation improve SLT performance?**
   - Compare original vs. augmented training sets
   - Measure BLEU/chrF improvements

2. **Can cross-lingual coupling via back-translation improve generalization?**
   - Test pivot language strategies (EN/DE/ES)
   - Evaluate on cross-domain protocols

3. **Do kinematic augmentations complement textual ones?**
   - Ablation studies with/without pose perturbations
   - Analyze robustness to occlusions and tracking noise

4. **Is a unified multilingual model viable?**
   - Train single model on 6 sign languages
   - Compare to language-specific baselines

---

## ðŸ“ Notes for AI Agents

- **Pose data**: Always check for NaN values (tracking failures) before processing
- **Text normalization**: Apply Unicode NFKC + lowercase before augmentation
- **Signformer format**: Uses gzip-pickled lists; feature size is 1024 (512 keypoints Ã— 2D)
- **Face keypoints**: Often removed for efficiency (reduces 543 â†’ 75 landmarks)
- **Streaming**: Large datasets use `stream_train_parts: true` to avoid OOM
- **SophiaG optimizer**: After install, edit `sophia/__init__.py` to remove problematic import

---

*Last updated: January 2026*

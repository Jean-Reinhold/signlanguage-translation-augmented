# Docker Compose for SLT Dataset Augmentation
# ============================================
#
# Runs back-translation augmentation for all 6 SLT datasets.
# Configured for GPT-4.1-mini with high rate limits:
#   - RPM: 5,000 requests/minute
#   - TPM: 2,000,000 tokens/minute
#
# Usage:
#   # Run only pending datasets (skip already completed):
#   docker compose -f docker-compose.augmentation.yml --profile pending up --build
#
#   # Run all datasets (including already completed):
#   docker compose -f docker-compose.augmentation.yml --profile all up --build
#
#   # Run specific dataset:
#   docker compose -f docker-compose.augmentation.yml up lsat
#

services:
  # ============================================
  # RWTH-PHOENIX-2014T (German Sign Language)
  # âœ… Completed: 7,096 â†’ 20,565 samples
  # ============================================
  rwth_phoenix:
    profiles: ["all"]
    build:
      context: .
      dockerfile: docker/augmentation/Dockerfile
    container_name: augment-rwth-phoenix
    env_file: .env
    tty: true
    environment:
      - DATASET=RWTH_PHOENIX_2014T
      - AZURE_OPENAI_RPM=5000
      - AZURE_OPENAI_TPM=2000000
    volumes:
      - /mnt/disk3Tb/slt-datasets:/mnt/disk3Tb/slt-datasets:ro
      - /mnt/disk3Tb/augmented-slt-datasets:/mnt/disk3Tb/augmented-slt-datasets
    command: ["bash", "-c", "echo 'ðŸ§¹ Cleaning output directory for RWTH_PHOENIX_2014T...' && rm -rf /mnt/disk3Tb/augmented-slt-datasets/RWTH_PHOENIX_2014T && echo 'ðŸš€ Starting augmentation for RWTH_PHOENIX_2014T...' && python -m src.augmentation.augment_dataset --dataset RWTH_PHOENIX_2014T --num-variants 2 --batch-size 500 --summary-report"]
    restart: "no"

  # ============================================
  # LSA-T (Argentine Sign Language - Spanish)
  # ðŸ”§ Fixed: Use LSAT (uppercase) directory
  # ============================================
  lsat:
    profiles: ["pending", "all"]
    build:
      context: .
      dockerfile: docker/augmentation/Dockerfile
    container_name: augment-lsat
    env_file: .env
    tty: true
    environment:
      - DATASET=LSAT
      - AZURE_OPENAI_RPM=5000
      - AZURE_OPENAI_TPM=2000000
    volumes:
      - /mnt/disk3Tb/slt-datasets:/mnt/disk3Tb/slt-datasets:ro
      - /mnt/disk3Tb/augmented-slt-datasets:/mnt/disk3Tb/augmented-slt-datasets
    command: ["bash", "-c", "echo 'ðŸ§¹ Cleaning output directory for LSAT...' && rm -rf /mnt/disk3Tb/augmented-slt-datasets/LSAT && echo 'ðŸš€ Starting augmentation for LSAT...' && python -m src.augmentation.augment_dataset --dataset LSAT --num-variants 2 --batch-size 500 --summary-report"]
    restart: "no"

  # ============================================
  # How2Sign (American Sign Language - English)
  # ðŸ”§ Fixed: Created annotations.csv (35,263 samples)
  # ============================================
  how2sign:
    profiles: ["pending", "all"]
    build:
      context: .
      dockerfile: docker/augmentation/Dockerfile
    container_name: augment-how2sign
    env_file: .env
    tty: true
    environment:
      - DATASET=How2Sign
      - AZURE_OPENAI_RPM=5000
      - AZURE_OPENAI_TPM=2000000
    volumes:
      - /mnt/disk3Tb/slt-datasets:/mnt/disk3Tb/slt-datasets:ro
      - /mnt/disk3Tb/augmented-slt-datasets:/mnt/disk3Tb/augmented-slt-datasets
    command: ["bash", "-c", "echo 'ðŸ§¹ Cleaning output directory for How2Sign...' && rm -rf /mnt/disk3Tb/augmented-slt-datasets/How2Sign && echo 'ðŸš€ Starting augmentation for How2Sign...' && python -m src.augmentation.augment_dataset --dataset How2Sign --num-variants 2 --batch-size 500 --summary-report"]
    depends_on:
      lsat:
        condition: service_completed_successfully
    restart: "no"

  # ============================================
  # ISL (Indian Sign Language - English)
  # ðŸ”§ Fixed: NaN text filtering (125,598 valid samples)
  # ============================================
  isl:
    profiles: ["pending", "all"]
    build:
      context: .
      dockerfile: docker/augmentation/Dockerfile
    container_name: augment-isl
    env_file: .env
    tty: true
    environment:
      - DATASET=ISL
      - AZURE_OPENAI_RPM=5000
      - AZURE_OPENAI_TPM=2000000
    volumes:
      - /mnt/disk3Tb/slt-datasets:/mnt/disk3Tb/slt-datasets:ro
      - /mnt/disk3Tb/augmented-slt-datasets:/mnt/disk3Tb/augmented-slt-datasets
    command: ["bash", "-c", "echo 'ðŸ§¹ Cleaning output directory for ISL...' && rm -rf /mnt/disk3Tb/augmented-slt-datasets/ISL && echo 'ðŸš€ Starting augmentation for ISL...' && python -m src.augmentation.augment_dataset --dataset ISL --num-variants 2 --batch-size 500 --summary-report"]
    depends_on:
      how2sign:
        condition: service_completed_successfully
    restart: "no"

  # ============================================
  # LSFB-CONT (Belgian French Sign Language)
  # ðŸ”§ Fixed: Created annotations.csv from JSON (5,727 samples)
  # ============================================
  lsfb_cont:
    profiles: ["pending", "all"]
    build:
      context: .
      dockerfile: docker/augmentation/Dockerfile
    container_name: augment-lsfb-cont
    env_file: .env
    tty: true
    environment:
      - DATASET=LSFB-CONT
      - AZURE_OPENAI_RPM=5000
      - AZURE_OPENAI_TPM=2000000
    volumes:
      - /mnt/disk3Tb/slt-datasets:/mnt/disk3Tb/slt-datasets:ro
      - /mnt/disk3Tb/augmented-slt-datasets:/mnt/disk3Tb/augmented-slt-datasets
    command: ["bash", "-c", "echo 'ðŸ§¹ Cleaning output directory for LSFB-CONT...' && rm -rf /mnt/disk3Tb/augmented-slt-datasets/LSFB-CONT && echo 'ðŸš€ Starting augmentation for LSFB-CONT...' && python -m src.augmentation.augment_dataset --dataset LSFB-CONT --num-variants 2 --batch-size 500 --summary-report"]
    depends_on:
      isl:
        condition: service_completed_successfully
    restart: "no"

  # ============================================
  # GSL (Greek Sign Language)
  # âœ… Already completed: 8,821 â†’ 12,058 samples
  # ============================================
  gsl:
    profiles: ["all"]
    build:
      context: .
      dockerfile: docker/augmentation/Dockerfile
    container_name: augment-gsl
    env_file: .env
    tty: true
    environment:
      - DATASET=GSL
      - AZURE_OPENAI_RPM=5000
      - AZURE_OPENAI_TPM=2000000
    volumes:
      - /mnt/disk3Tb/slt-datasets:/mnt/disk3Tb/slt-datasets:ro
      - /mnt/disk3Tb/augmented-slt-datasets:/mnt/disk3Tb/augmented-slt-datasets
    command: ["bash", "-c", "echo 'ðŸ§¹ Cleaning output directory for GSL...' && rm -rf /mnt/disk3Tb/augmented-slt-datasets/GSL && echo 'ðŸš€ Starting augmentation for GSL...' && python -m src.augmentation.augment_dataset --dataset GSL --num-variants 2 --batch-size 500 --summary-report"]
    restart: "no"

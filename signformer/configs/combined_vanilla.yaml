# =============================================================================
# COMBINED VANILLA BASELINE TRAINING - v6 (TRUE INTERLEAVED BATCHING)
# =============================================================================
#
# ⚠️  WARNING: DISK SPACE CRITICAL - Only 29GB free!
#     Before training, consider deleting old merged files.
#     See docs/RESOURCE_CONSTRAINTS.md for details.
#
# =============================================================================
# 
# Purpose: Train Signformer on all 5 datasets WITHOUT augmentation
# This serves as the baseline to compare against augmented training.
#
# v6 IMPROVEMENTS (BEST PRACTICE MULTILINGUAL TRAINING):
#   - TRUE INTERLEAVED BATCHING: Each batch contains samples from ALL languages
#   - This prevents catastrophic forgetting (the main issue in v5b)
#   - Temperature-weighted sampling (T=5) balances high/low resource languages
#   - Per-language validation metrics continue from v5
#   - Every gradient update sees all 5 sign languages
#
# Architecture: EXACTLY MATCHES ORIGINAL SIGNFORMER (lightweight ~6M params)
#   - 1-layer encoder/decoder (as in paper)
#   - 256 hidden size (as in paper)
#   - ORIGINAL hyperparameters (lr, batch, warmup, patience)
#
# Datasets (all 1024 features):
#   - RWTH-PHOENIX-2014T: German Sign Language (DGS) -> German
#   - GSL: Greek Sign Language -> Greek
#   - LSAT: Argentine Sign Language (LSA) -> Spanish
#   - How2Sign-1024: American Sign Language (ASL) -> English (padded)
#   - LSFB-CONT-1024: French Belgian Sign Language -> French (padded)
#
# Hardware Target:
#   - GPU: NVIDIA RTX 3090 (24GB VRAM)
#   - RAM: 31GB
#   - CPU: 4 cores
#
# =============================================================================

name: combined_vanilla_baseline

data:
    data_path: ../PHOENIX2014T
    version: phoenix_2014_trans
    sgn: sign
    txt: text
    gls: gloss
    
    # Combined training data (vanilla only)
    train:
      - RWTH_PHOENIX_2014T.pami0.train
      - GSL.pami0.train
      - LSAT.pami0.train
      - How2Sign-1024.pami0.train      # Padded OpenPose -> 1024
      - LSFB-CONT-1024.pami0.train     # Padded custom -> 1024
    
    # Validation: Combined (for backward compatibility - uses PHOENIX)
    dev:
      - RWTH_PHOENIX_2014T.pami0.val
    
    # Per-language validation sets (v4 feature)
    # Each language is validated separately and metrics are logged individually
    dev_per_language:
      PHOENIX:  RWTH_PHOENIX_2014T.pami0.val    # German Sign Language -> German
      GSL:      GSL.pami0.val                   # Greek Sign Language -> Greek  
      LSAT:     LSAT.pami0.val                  # Argentine Sign Language -> Spanish
      How2Sign: How2Sign-1024.pami0.val         # ASL -> English
      LSFB:     LSFB-CONT-1024.pami0.val        # French Belgian SL -> French
    
    # Test: evaluate on all datasets
    test:
      - RWTH_PHOENIX_2014T.pami0.test
      - GSL.pami0.test
      - LSAT.pami0.test
      - How2Sign-1024.pami0.test
      - LSFB-CONT-1024.pami0.test
    
    feature_size: 1024
    level: word
    txt_lowercase: true
    # Prefix target text with language tags to guide multilingual decoding
    prepend_lang_token: true
    lang_token_map:
      RWTH_PHOENIX_2014T: "<lang=de>"
      GSL: "<lang=el>"
      LSAT: "<lang=es>"
      How2Sign: "<lang=en>"
      LSFB-CONT: "<lang=fr>"
    max_sent_length: 400
    random_train_subset: -1
    random_dev_subset: -1
    multimodal: 0.0
    
    # Memory-efficient streaming
    stream_train_parts: true
    stream_chunk_size: 3000           # Between original 5000 and conservative 2500
    stream_dev_parts: true
    stream_test_parts: true
    
    # Temperature-based sampling for multilingual training (v6 - interleaved)
    # Controls how samples are distributed WITHIN each batch:
    #   T=1: proportional to dataset size (larger datasets get more samples per batch)
    #   T=5: balanced (standard for multilingual, as in mT5/mBART)
    #   T=∞: uniform (equal samples from each language per batch)
    # 
    # v6 KEY IMPROVEMENT: Every batch contains samples from ALL languages!
    # This ensures every gradient update improves all 5 languages simultaneously.
    sampling_temperature: 5.0
    # Keep all datasets active across the epoch by sampling with replacement
    interleaved_repeat: true
    # Set a fixed number of interleaved batches per epoch (adjust if dataset sizes change)
    interleaved_epoch_batches: 1700

training:
    # Checkpoint management
    reset_best_ckpt: true
    reset_scheduler: true
    reset_optimizer: true
    random_seed: 42
    
    # Output directory (mounted volume)
    model_dir: "/mnt/experiments/combined-vanilla-v6"
    
    # Loss configuration (translation only, no gloss recognition)
    recognition_loss_weight: 0.0
    translation_loss_weight: 1.0
    
    # Evaluation
    eval_metric: bleu
    early_stopping_metric: eval_metric
    
    # Optimizer: SophiaG (ORIGINAL Signformer settings)
    optimizer: sophiag
    learning_rate: 0.0004             # ORIGINAL: exactly as in sign.yaml
    betas:
      - 0.95                          # Original beta1
      - 0.99                          # Original beta2
    weight_decay: 0.003               # Original weight decay
    
    # Batch configuration (ORIGINAL)
    batch_size: 32                    # ORIGINAL: exactly as in sign.yaml
    batch_type: sentence
    batch_multiplier: 1               # ORIGINAL: no gradient accumulation
    translation_normalization: batch
    
    # Training schedule (ORIGINAL)
    epochs: 1000                      # Extended for multilingual
    scheduling: plateau
    learning_rate_min: 1.0e-06        # ORIGINAL
    patience: 10                      # ORIGINAL
    decrease_factor: 0.8              # ORIGINAL
    warmup_step: 1000                 # ORIGINAL
    
    # Regularization
    label_smoothing: 0.1              # Same as original
    clip_grad_norm: 1.0               # Keep gradient clipping for stability
    
    # SophiaG specific (from original)
    lr_s_dim_model: 256
    K: 2
    
    # Logging and checkpoints (OPTIMIZED FOR SPEED)
    logging_freq: 200                 # Less frequent logging
    validation_freq: 500              # Validate every 500 steps (was 100)
    num_valid_log: 6                  # Same as original
    keep_last_ckpts: 1                # Minimal: only best + 1 recent
    
    # Beam search during validation
    eval_recognition_beam_size: 1
    eval_translation_beam_size: 1
    eval_translation_beam_alpha: -1
    translation_max_output_length: 30 # Same as original
    
    # Runtime
    overwrite: true
    shuffle: true
    use_cuda: true

# =============================================================================
# MODEL: ORIGINAL SIGNFORMER ARCHITECTURE (~6M params)
# =============================================================================
# Exactly matches original Signformer - proven to work on PHOENIX
# =============================================================================
model:
    initializer: xavier
    bias_initializer: zeros
    init_gain: 1.0
    embed_initializer: xavier
    embed_init_gain: 1.0
    tied_softmax: false
    cope: false
    
    encoder:
        type: transformer
        num_layers: 1                 # ORIGINAL: 1 layer
        num_heads: 8
        embeddings:
            embedding_dim: 256        # ORIGINAL
            scale: false
            dropout: 0.1              # ORIGINAL
            norm_type: batch
            activation_type: softsign
        hidden_size: 256              # ORIGINAL
        ff_size: 1024                 # ORIGINAL
        dropout: 0.1                  # ORIGINAL
    
    decoder:
        type: transformer
        num_layers: 1                 # ORIGINAL: 1 layer
        num_heads: 8
        embeddings:
            embedding_dim: 256        # ORIGINAL
            scale: false
            dropout: 0.1              # ORIGINAL
            norm_type: batch
            activation_type: softsign
        hidden_size: 256              # ORIGINAL
        ff_size: 1024                 # ORIGINAL
        dropout: 0.1                  # ORIGINAL

testing:
    recognition_beam_sizes:
      - 1
    translation_beam_sizes:
      - 1
      - 5
    translation_beam_alphas:
      - -1
      - 0
      - 1

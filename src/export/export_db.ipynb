{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check original format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/disk3Tb/exported-slt-datasets/RWTH_PHOENIX_2014T.pami0.test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loaded_object\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# original_dataset = load_dataset_file(\"/home/pdalbianco/Github/Signformer/PHOENIX2014T/phoenix14t.pami0.dev\")\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m original_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/mnt/disk3Tb/exported-slt-datasets/RWTH_PHOENIX_2014T.pami0.test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m, in \u001b[0;36mload_dataset_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dataset_file\u001b[39m(filename):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mgzip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m         loaded_object \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loaded_object\n",
      "File \u001b[0;32m/usr/lib/python3.10/gzip.py:58\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m gz_mode \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike)):\n\u001b[0;32m---> 58\u001b[0m     binary_file \u001b[38;5;241m=\u001b[39m \u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgz_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     60\u001b[0m     binary_file \u001b[38;5;241m=\u001b[39m GzipFile(\u001b[38;5;28;01mNone\u001b[39;00m, gz_mode, compresslevel, filename)\n",
      "File \u001b[0;32m/usr/lib/python3.10/gzip.py:174\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    172\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/disk3Tb/exported-slt-datasets/RWTH_PHOENIX_2014T.pami0.test'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "\n",
    "def load_dataset_file(filename):\n",
    "    with gzip.open(filename, \"rb\") as f:\n",
    "        loaded_object = pickle.load(f)\n",
    "        return loaded_object\n",
    "\n",
    "# original_dataset = load_dataset_file(\"/home/pdalbianco/Github/Signformer/PHOENIX2014T/phoenix14t.pami0.dev\")\n",
    "original_dataset = load_dataset_file(\"/mnt/disk3Tb/exported-slt-datasets/RWTH_PHOENIX_2014T.pami0.test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slt_datasets.SLTDataset import SLTDataset\n",
    "\n",
    "\n",
    "DATASET = \"RWTH_PHOENIX_2014T\"\n",
    "# DATASET = \"ISL\"\n",
    "DATA_DIR = \"/mnt/disk3Tb/slt-datasets/\"\n",
    "INPUT_MODE = \"pose\"\n",
    "OUTPUT_MODE = \"text\"\n",
    "OUTPUT_DIR = \"/mnt/disk3Tb/exported-slt-datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "from slt_datasets.SLTDataset import SLTDataset\n",
    "\n",
    "\n",
    "def format_pose(pose):\n",
    "\tpose = pose[:, 0, :, :2]\n",
    "\treturn pose.reshape(pose.shape[0], -1)\n",
    "\n",
    "def export_dataset(dataset: SLTDataset, output_path: str):\n",
    "\tsamples = []\n",
    "\tfor i in tqdm(range(len(dataset))):\n",
    "\t\tpose = format_pose(dataset[i][0])\n",
    "\t\t# pose is pytorch tensor shape (N, 1086), remove items from position 34 to 95 (face keypoints) to make it (N, 1024)\n",
    "\t\tpose = torch.cat((pose[:, :34], pose[:, 96:]), dim=1)\n",
    "\t\tpose = torch.nan_to_num(pose, nan=0.0)\n",
    "\t\ttext = dataset.get_item_raw(i)[1]\n",
    "\t\tname = dataset.annotations.iloc[i]['id']\n",
    "\t\tsamples.append({\n",
    "\t\t\t\"sign\": pose,\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"gloss\": \"\",\n",
    "\t\t\t\"signer\": \"\",\n",
    "\t\t\t\"name\": name\n",
    "\t\t})\n",
    "\tprint(f\"Saving dataset to {output_path}\")\n",
    "\twith gzip.open(output_path, \"wb\") as f:\n",
    "\t\tpickle.dump(samples, f)\t\t\n",
    "\tprint(f\"Dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata for dataset: RWTH-PHOENIX-Weather 2014 T: Parallel Corpus of Sign Language Video, Gloss and Translation\n",
      "Loaded train annotations at /mnt/disk3Tb/slt-datasets/RWTH_PHOENIX_2014T/annotations.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7096/7096 [00:00<00:00, 23226.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded correctly\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7096/7096 [02:06<00:00, 56.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to /mnt/disk3Tb/exported-slt-datasets/RWTH_PHOENIX_2014T.pami0.train\n",
      "Dataset saved to /mnt/disk3Tb/exported-slt-datasets/RWTH_PHOENIX_2014T.pami0.train\n",
      "Loaded metadata for dataset: RWTH-PHOENIX-Weather 2014 T: Parallel Corpus of Sign Language Video, Gloss and Translation\n",
      "Loaded val annotations at /mnt/disk3Tb/slt-datasets/RWTH_PHOENIX_2014T/annotations.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 519/519 [00:00<00:00, 1126.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded correctly\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 519/519 [00:08<00:00, 58.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to /mnt/disk3Tb/exported-slt-datasets/RWTH_PHOENIX_2014T.pami0.val\n",
      "Dataset saved to /mnt/disk3Tb/exported-slt-datasets/RWTH_PHOENIX_2014T.pami0.val\n",
      "Loaded metadata for dataset: RWTH-PHOENIX-Weather 2014 T: Parallel Corpus of Sign Language Video, Gloss and Translation\n",
      "Loaded test annotations at /mnt/disk3Tb/slt-datasets/RWTH_PHOENIX_2014T/annotations.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 642/642 [00:00<00:00, 34597.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded correctly\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 642/642 [00:11<00:00, 57.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to /mnt/disk3Tb/exported-slt-datasets/RWTH_PHOENIX_2014T.pami0.test\n",
      "Dataset saved to /mnt/disk3Tb/exported-slt-datasets/RWTH_PHOENIX_2014T.pami0.test\n"
     ]
    }
   ],
   "source": [
    "for SPLIT in [\"train\", \"val\", \"test\"]:\n",
    "\tdataset = SLTDataset(\n",
    "\t\tdata_dir=DATA_DIR + DATASET,\n",
    "\t\tinput_mode=INPUT_MODE,\n",
    "\t\toutput_mode=OUTPUT_MODE,\n",
    "\t\tsplit=SPLIT\n",
    "\t)\n",
    "\texport_dataset(dataset, f\"{OUTPUT_DIR}/{DATASET}.pami0.{SPLIT}\")\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISL has no anottations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# utils/export.py  (put it wherever you prefer)\n",
    "# ------------------------------------------------------\n",
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "def format_pose(pose):\n",
    "    pose = pose[:, 0, :, :2]\n",
    "    return pose.reshape(pose.shape[0], -1)\n",
    "\n",
    "\n",
    "def _write_chunk(chunk, out_path, part_idx):\n",
    "    \"\"\"Pickle-gzip one chunk to â€¦/file.part-<idx>\"\"\"\n",
    "    part_path = f\"{out_path}.part-{part_idx}\"\n",
    "    Path(part_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with gzip.open(part_path, \"wb\") as f:\n",
    "        pickle.dump(chunk, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"  âœ” saved {len(chunk):5d} samples â†’ {part_path}\")\n",
    "\n",
    "\n",
    "def export_subset(dataset, indices, output_path, *, chunk_size=5_000):\n",
    "    \"\"\"\n",
    "    Write <indices> to <output_path>.part-* in chunks of <chunk_size>.\n",
    "    The file format of every part is exactly the same you used before\n",
    "    (a pickled Python list of dicts).\n",
    "    \"\"\"\n",
    "    buffer, part = [], 0\n",
    "    print(f\"Exporting {len(indices)} samples â†’ {output_path}.part-* \"\n",
    "          f\"(chunk_size={chunk_size})\")\n",
    "\n",
    "    for i in tqdm(indices):\n",
    "        pose = format_pose(dataset[i][0])\n",
    "        pose = torch.cat((pose[:, :34], pose[:, 96:]), dim=1)\n",
    "        pose = torch.nan_to_num(pose, nan=0.0)\n",
    "\n",
    "        text = dataset.get_item_raw(i)[1]\n",
    "        name = dataset.annotations.iloc[i][\"id\"]\n",
    "\n",
    "        buffer.append(\n",
    "            {\"sign\": pose, \"text\": text, \"gloss\": \"\", \"signer\": \"\", \"name\": name}\n",
    "        )\n",
    "\n",
    "        if len(buffer) == chunk_size:\n",
    "            _write_chunk(buffer, output_path, part)\n",
    "            part += 1\n",
    "            buffer = []\n",
    "\n",
    "    if buffer:  # last, possibly smaller chunk\n",
    "        _write_chunk(buffer, output_path, part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata for dataset: ISLTranslate\n",
      "Loaded  annotations at /mnt/disk3Tb/slt-datasets/ISL/annotations.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125856/125856 [00:06<00:00, 18408.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 17534 files out of 125856 (13.93%)\n",
      "\n",
      "Total: 108322  âžœ  train 75825  test 21664  val 10833\n",
      "Exporting 10833 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.val.part-* (chunk_size=5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 5001/10833 [06:36<37:27:33, 23.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.val.part-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 10003/10833 [13:42<3:20:44, 14.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.val.part-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10833/10833 [14:14<00:00, 12.68it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved   833 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.val.part-2\n",
      "Exporting 21664 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.test.part-* (chunk_size=5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 5002/21664 [06:38<66:44:01, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.test.part-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10004/21664 [13:41<43:37:53, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.test.part-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15000/21664 [20:21<31:55:14, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.test.part-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 20003/21664 [26:53<6:14:42, 13.54s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.test.part-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21664/21664 [27:55<00:00, 12.93it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  1664 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.test.part-4\n",
      "Exporting 75825 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-* (chunk_size=5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 5003/75825 [06:32<260:13:33, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 10005/75825 [13:13<230:30:11, 12.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 15003/75825 [19:48<199:01:33, 11.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–‹       | 20003/75825 [26:20<207:51:47, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 25003/75825 [33:03<198:28:15, 14.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 30004/75825 [39:42<176:11:33, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 35000/75825 [46:35<253:50:16, 22.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 40003/75825 [53:17<129:34:19, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 45003/75825 [1:00:08<118:48:14, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 50002/75825 [1:06:48<112:30:30, 15.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 55003/75825 [1:13:27<97:14:16, 16.81s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 60002/75825 [1:20:06<71:40:05, 16.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 65004/75825 [1:26:51<38:25:51, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 70001/75825 [1:33:36<35:15:46, 21.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 75004/75825 [1:40:22<3:24:41, 14.96s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved  5000 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75825/75825 [1:40:58<00:00, 12.52it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ” saved   825 samples â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train.part-15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Load the full ISL dataset (no split argument)\n",
    "# ------------------------------------------------------------------\n",
    "DATASET = \"ISL\"\n",
    "\n",
    "isl_full = SLTDataset(\n",
    "    data_dir      = DATA_DIR + DATASET,\n",
    "    input_mode    = INPUT_MODE,\n",
    "    output_mode   = OUTPUT_MODE,   # <- no split keyword passed\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Create a reproducible shuffled index list\n",
    "# ------------------------------------------------------------------\n",
    "rng = np.random.default_rng(seed=42)          # fixed seed = reproducible\n",
    "shuffled_idx = rng.permutation(len(isl_full))\n",
    "\n",
    "# 70 / 20 / 10  split points\n",
    "n_total     = len(shuffled_idx)\n",
    "cut1        = int(0.70 * n_total)\n",
    "cut2        = int(0.90 * n_total)             # 70 % + 20 % = 90 %\n",
    "\n",
    "idx_train   = shuffled_idx[:cut1]\n",
    "idx_test    = shuffled_idx[cut1:cut2]\n",
    "idx_val     = shuffled_idx[cut2:]\n",
    "\n",
    "print(f\"Total: {n_total}  âžœ  train {len(idx_train)}  \"\n",
    "      f\"test {len(idx_test)}  val {len(idx_val)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Export each split\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "export_subset(isl_full, idx_val,\n",
    "              f\"{OUTPUT_DIR}/{DATASET}.pami0.val\")\n",
    "export_subset(isl_full, idx_test,\n",
    "              f\"{OUTPUT_DIR}/{DATASET}.pami0.test\")\n",
    "export_subset(isl_full, idx_train,\n",
    "              f\"{OUTPUT_DIR}/{DATASET}.pami0.train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/concat.py\n",
    "import glob\n",
    "import gzip\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def merge_dataset_parts(prefix: str,\n",
    "                        output_path: str,\n",
    "                        *,\n",
    "                        buffer_limit: int = 5_000) -> None:\n",
    "    \"\"\"\n",
    "    Concatenate every  <prefix>.part-*  into one gzip file at <output_path>\n",
    "    WITHOUT loading the whole dataset into memory.\n",
    "\n",
    "    â€¢ buffer_limit : number of samples kept in RAM before they get written out.\n",
    "    \"\"\"\n",
    "    part_paths = sorted(glob.glob(f\"{prefix}.part-*\"))\n",
    "    if not part_paths:\n",
    "        raise FileNotFoundError(f\"No parts found for prefix '{prefix}'\")\n",
    "\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    buffer = []\n",
    "\n",
    "    print(f\"Writing {len(part_paths)} parts â†’ {output_path}\")\n",
    "    with gzip.open(output_path, \"wb\") as fout:\n",
    "        for p in tqdm(part_paths, desc=\"Merging\"):\n",
    "            with gzip.open(p, \"rb\") as fpart:\n",
    "                part_samples = pickle.load(fpart)        # one chunk\n",
    "            buffer.extend(part_samples)\n",
    "\n",
    "            if len(buffer) >= buffer_limit:\n",
    "                pickle.dump(buffer, fout, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                buffer.clear()                          # free RAM\n",
    "\n",
    "        # last, possibly smaller remainder\n",
    "        if buffer:\n",
    "            pickle.dump(buffer, fout, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"Done âœ”\")\n",
    "\n",
    "\n",
    "def load_dataset_file(path: str):\n",
    "    \"\"\"\n",
    "    Load a (possibly multi-pickle) gzip file produced by merge_dataset_parts.\n",
    "    Works for the original one-pickle files as well.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    with gzip.open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                chunk = pickle.load(f)\n",
    "            except EOFError:\n",
    "                break\n",
    "            samples.extend(chunk)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 5 parts â†’ /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [16:44<00:00, 200.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done âœ”\n",
      "21664\n",
      "dict_keys(['sign', 'text', 'gloss', 'signer', 'name'])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Step 1:  Merge all parts of one split\n",
    "# ------------------------------------------------------------------\n",
    "merge_dataset_parts(\"/mnt/disk3Tb/exported-slt-datasets/ISL.pami0.val\",\n",
    "                    \"/mnt/disk3Tb/exported-slt-datasets/ISL.pami0.val\",\n",
    "                    buffer_limit=5_000)      # keep â‰¤ 5 k samples in RAM\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Step 2:  Load the merged file later\n",
    "# ------------------------------------------------------------------\n",
    "train_samples = load_dataset_file(\n",
    "        \"/mnt/disk3Tb/exported-slt-datasets/ISL.pami0.val\")\n",
    "\n",
    "print(len(train_samples))         # 75 825\n",
    "print(train_samples[0].keys())    # dict keys unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating 5 parts â†’ /mnt/disk3Tb/exported-slt-datasets/ISL-MERGED.pami0.test.gz\n",
      "Done âœ”\n",
      "Concatenating 3 parts â†’ /mnt/disk3Tb/exported-slt-datasets/ISL-MERGED.pami0.val.gz\n",
      "Done âœ”\n",
      "Concatenating 16 parts â†’ /mnt/disk3Tb/exported-slt-datasets/ISL-MERGED.pami0.train.gz\n",
      "Done âœ”\n"
     ]
    }
   ],
   "source": [
    "# utils/concat_raw.py\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Sequence\n",
    "\n",
    "def concat_gzip_parts(prefix: str,\n",
    "                      output_path: str,\n",
    "                      *,\n",
    "                      chunk_size: int = 512 * 1024) -> None:\n",
    "    \"\"\"\n",
    "    Concatenate every <prefix>.part-* into a single gzip file\n",
    "    without decompressing anything.\n",
    "\n",
    "    Each .part-* must be a complete gzip member (which is true for your exporter).\n",
    "    \"\"\"\n",
    "    part_paths: Sequence[str] = sorted(glob.glob(f\"{prefix}.part-*\"))\n",
    "    if not part_paths:\n",
    "        raise FileNotFoundError(f\"No parts found for prefix '{prefix}'\")\n",
    "\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Concatenating {len(part_paths)} parts â†’ {output_path}\")\n",
    "    with open(output_path, \"wb\") as fout:\n",
    "        for path in part_paths:\n",
    "            with open(path, \"rb\") as fin:\n",
    "                shutil.copyfileobj(fin, fout, length=chunk_size)\n",
    "    print(\"Done âœ”\")\n",
    "\n",
    "concat_gzip_parts(\"/mnt/disk3Tb/exported-slt-datasets/ISL.pami0.test\", \"/mnt/disk3Tb/exported-slt-datasets/ISL-MERGED.pami0.test.gz\")\n",
    "concat_gzip_parts(\"/mnt/disk3Tb/exported-slt-datasets/ISL.pami0.val\", \"/mnt/disk3Tb/exported-slt-datasets/ISL-MERGED.pami0.val.gz\")\n",
    "concat_gzip_parts(\"/mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train\", \"/mnt/disk3Tb/exported-slt-datasets/ISL-MERGED.pami0.train.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge of pre-exported datasets â†’ MERGE.pami0.{train,val,test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ adding /mnt/disk3Tb/exported-slt-datasets/GSL.pami0.train\n",
      "â†’ adding /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.train\n",
      "â†’ adding /mnt/disk3Tb/exported-slt-datasets/LSAT.pami0.train\n",
      "â†’ adding /mnt/disk3Tb/exported-slt-datasets/RWTH_PHOENIX_2014T.pami0.train\n",
      "âœ“ wrote /mnt/disk3Tb/exported-slt-datasets/MERGE.pami0.train\n",
      "â†’ adding /mnt/disk3Tb/exported-slt-datasets/GSL.pami0.val\n",
      "â†’ adding /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.val\n",
      "â†’ adding /mnt/disk3Tb/exported-slt-datasets/LSAT.pami0.val\n",
      "â†’ adding /mnt/disk3Tb/exported-slt-datasets/RWTH_PHOENIX_2014T.pami0.val\n",
      "âœ“ wrote /mnt/disk3Tb/exported-slt-datasets/MERGE.pami0.val\n",
      "â†’ adding /mnt/disk3Tb/exported-slt-datasets/GSL.pami0.test\n",
      "â†’ adding /mnt/disk3Tb/exported-slt-datasets/ISL.pami0.test\n",
      "â†’ adding /mnt/disk3Tb/exported-slt-datasets/LSAT.pami0.test\n",
      "â†’ adding /mnt/disk3Tb/exported-slt-datasets/RWTH_PHOENIX_2014T.pami0.test\n",
      "âœ“ wrote /mnt/disk3Tb/exported-slt-datasets/MERGE.pami0.test\n",
      "ðŸš€ MERGE.pami0.{train,val,test} listos en /mnt/disk3Tb/exported-slt-datasets\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Merge of pre-exported datasets â†’ MERGE.pami0.{train,val,test}\n",
    "# ================================================================\n",
    "import gzip\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directory where the .pami0.* files live\n",
    "OUTPUT_DIR = Path(OUTPUT_DIR)        # usa la misma variable del notebook; ajusta si fuera necesario\n",
    "\n",
    "DATASETS   = [\"GSL\", \"ISL\", \"LSAT\", \"RWTH_PHOENIX_2014T\"]   # phoenix14t.* se ignoran\n",
    "SPLITS     = [\"train\", \"val\", \"test\"]\n",
    "BUFFER_LEN = 5_000        # nÂº de ejemplos a escribir por chunk (ajusta a tu RAM)\n",
    "\n",
    "def stream_samples(path: Path):\n",
    "    \"\"\"Yield every sample stored (list o unitario) inside a gzip-pickle file.\"\"\"\n",
    "    with gzip.open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                obj = pickle.load(f)\n",
    "            except EOFError:\n",
    "                break\n",
    "            if isinstance(obj, list):\n",
    "                yield from obj\n",
    "            else:\n",
    "                yield obj\n",
    "\n",
    "def merge_split(split: str):\n",
    "    out_path = OUTPUT_DIR / f\"MERGE.pami0.{split}\"\n",
    "    buffer = []\n",
    "    with gzip.open(out_path, \"wb\") as fout:\n",
    "        for ds in DATASETS:\n",
    "            in_path = OUTPUT_DIR / f\"{ds}.pami0.{split}\"\n",
    "            print(f\"â†’ adding {in_path}\")\n",
    "            for sample in stream_samples(in_path):\n",
    "                buffer.append(sample)\n",
    "                if len(buffer) >= BUFFER_LEN:\n",
    "                    pickle.dump(buffer, fout, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    buffer.clear()\n",
    "        if buffer:                                 # Ãºltimo resto\n",
    "            pickle.dump(buffer, fout, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"âœ“ wrote {out_path}\")\n",
    "\n",
    "for split in SPLITS:\n",
    "    merge_split(split)\n",
    "\n",
    "print(\"ðŸš€ MERGE.pami0.{train,val,test} listos en\", OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
